{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74edb6e1-0a85-4e69-82db-0fc968211af1",
   "metadata": {},
   "source": [
    "This notebook benchmarks the adaptive gradient solver and the exponential Euler solver to compare their training times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8193b238-e02c-4a7a-934e-149af906b9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "import time\n",
    "\n",
    "from tabulate import tabulate\n",
    "import torch\n",
    "import torchvision as tv\n",
    "\n",
    "import exponential_euler_solver as euler\n",
    "\n",
    "N_TRAIN = [64, 256]\n",
    "N_CLASSES = [2, 4, 10]\n",
    "N_SEEDS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c217c1-5830-4860-8385-0f9a3dd78c62",
   "metadata": {},
   "source": [
    "We use CIFAR-10 as our training data, flattened into 3072-dimensional vectors. We vary the number of classes and the size of the subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e812d8ec-db35-4283-8a8b-9dd4ad9058f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "all_ds = tv.datasets.CIFAR10('.', download=True)\n",
    "all_images = torch.from_numpy(all_ds.data).float().flatten(1).cuda() / 255\n",
    "all_labels = torch.tensor(all_ds.targets).cuda()\n",
    "\n",
    "def build_dataset(n_train: int, n_classes: int, seed: int = 0) -> torch.utils.data.TensorDataset:\n",
    "    torch.manual_seed(seed)\n",
    "    mask = (all_labels < n_classes)\n",
    "    images, labels = all_images[mask], all_labels[mask]\n",
    "    idxs = torch.randperm(len(labels), device='cuda')[:n_train]\n",
    "    return torch.utils.data.TensorDataset(images[idxs], labels[idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c92019-f7b9-49ed-9f48-3a65569b0a31",
   "metadata": {},
   "source": [
    "We use a 6-layer multilayer perceptron as our network. We use the exponential linear unit (ELU) as our activation function to prevent problems due to non-smoothness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "189d8b03-fd4c-4fb8-adc8-2ee0c6c23794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(n_classes: int) -> torch.nn.Sequential:\n",
    "    net = [torch.nn.Linear(3072, 512), torch.nn.ELU()]\n",
    "    for _ in range(4):\n",
    "        net += [torch.nn.Linear(512, 512), torch.nn.ELU()]\n",
    "    net += [torch.nn.Linear(512, n_classes)]\n",
    "    return torch.nn.Sequential(*net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9874ebf3-8c96-4761-a6e5-2cda138ef884",
   "metadata": {},
   "source": [
    "Now let's train the networks. Note that we use some learning rate warmup with the adaptive gradient solver: we found this was necessary to ensure that the loss declined monotonically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37eebb7b-b654-46fc-a438-8258a67a0102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  # Train    # Classes    Exponential Euler Time    Adaptive Gradient Time\n",
      "---------  -----------  ------------------------  ------------------------\n",
      "       64            2                   4.38303                   11.4608\n",
      "       64            4                  14.0392                    13.9129\n",
      "       64           10                  70.4531                    12.2033\n",
      "      256            2                  10.3577                    68.1163\n",
      "      256            4                  33.0347                    88.1126\n",
      "      256           10                 182.06                      81.9457\n"
     ]
    }
   ],
   "source": [
    "table = []\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for n_train in N_TRAIN:\n",
    "    for n_classes in N_CLASSES:\n",
    "        adapt_times, euler_times = [], []\n",
    "        for seed in range(N_SEEDS):\n",
    "            ds = build_dataset(n_train, n_classes, seed)\n",
    "            net = build_network(n_classes)\n",
    "            loss_func = euler.LossFunction(ds, criterion, net, batch_size=len(ds))\n",
    "            torch.manual_seed(seed)\n",
    "            params = loss_func.initialize_parameters()\n",
    "            solver = euler.ExponentialEulerSolver(\n",
    "                params, loss_func, 0.01, n_classes - 1\n",
    "            )\n",
    "            loss = float('inf')\n",
    "\n",
    "            start_time = time.perf_counter()\n",
    "            while loss > 0.01:\n",
    "                new_loss = solver.step().loss\n",
    "                assert new_loss < loss\n",
    "                loss = new_loss\n",
    "            euler_times.append(time.perf_counter() - start_time)\n",
    "\n",
    "            torch.manual_seed(seed)\n",
    "            params = loss_func.initialize_parameters()\n",
    "            solver = euler.AdaptiveGradientDescent(\n",
    "                params, loss_func, 0.01, warmup_iters=10, warmup_factor=0.01,\n",
    "            )\n",
    "            loss = float('inf')\n",
    "\n",
    "            start_time = time.perf_counter()\n",
    "            while loss > 0.01:\n",
    "                new_loss = solver.step().loss\n",
    "                assert new_loss < loss\n",
    "                loss = new_loss\n",
    "            adapt_times.append(time.perf_counter() - start_time)\n",
    "\n",
    "        table.append([n_train, n_classes, mean(euler_times), mean(adapt_times)])\n",
    "\n",
    "headers=['# Train', '# Classes', 'Exponential Euler Time', 'Adaptive Gradient Time']\n",
    "print(tabulate(table, headers=headers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c1194f-7ea2-4c51-b6f0-36ac7e880a12",
   "metadata": {},
   "source": [
    "This table was generated on a LambdaLabs A100 instance. We see that the exponential Euler solver is better when the number of classes is small. It also scales better as the size of the dataset increases, so if we kept increasing the dataset size, we would expect the exponential Euler solver to eventually beat the adaptive gradient solver. This is caused by the fact that the exponential Euler solver needs to calculate as many eigenvalue-eigenvector pairs as the network has outputs, while the adaptive gradient solver only needs to calculate one. As a result, the adaptive gradient solver's steps are faster if the network has more than a handful of classes, but each exponential Euler solver step can go further so it needs fewer of them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
